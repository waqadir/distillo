# Example configuration for on-policy training
# This demonstrates on-policy distillation with KL supervision

server:
  base_url: "http://localhost:9000"
  api_key: null
  verify_tls: true
  request_timeout: 120.0

job:
  # Model configuration - student model for on-policy training
  model:
    provider: "vllm"
    name: "Qwen/Qwen3-VL-8B-Instruct"
    parameters:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.85

  # Data source
  source:
    dataset: "HuggingFaceH4/ultrachat_200k"
    split: "train_sft"
    field: "messages"
    streaming: false

  # Generation configuration - with on-policy settings
  generation:
    duplications: 1
    max_batch_size: 16
    seed: 42
    parameters:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 2048

    # Enable on-policy training
    on_policy: true
    on_policy_options:
      teacher: "gpt-5"  # Teacher model for KL supervision
      api_key: "your-openai-api-key"
      learning_rate: 5.0e-5
      groups_per_batch: 512
      group_size: 4
      max_tokens: 4096
      lora_rank: 16
      loss_fn: "importance_sampling"  # or "ppo"
      kl_penalty_coef: 0.1
      compute_post_kl: true
      eval_every: 100
      save_every: 500

  # Output configuration
  output:
    mode: "return"
    format: "jsonl"

  metadata:
    experiment_name: "on_policy_training"
    student_model: "Qwen3-VL-8B"
    teacher_model: "gpt-5"
