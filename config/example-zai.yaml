# Example configuration for Z.AI (GLM) backend
# Z.AI provides GLM models with OpenAI-compatible API

server:
  base_url: "http://localhost:9000"
  api_key: null
  verify_tls: true
  request_timeout: 60.0

job:
  # Model configuration - using Z.AI API
  model:
    provider: "zai"  # or "glm"
    name: "glm-4.6"  # Options: glm-4.6, glm-4.5, glm-4.5-flash, etc.
    parameters:
      api_key: "your-zai-api-key"  # Or set ZAI_API_KEY env var

  # Data source
  source:
    dataset: "tatsu-lab/alpaca"
    split: "train"
    field: "instruction"
    streaming: false

  # Generation configuration
  generation:
    duplications: 1
    max_batch_size: 16  # Smaller batches for API calls
    seed: null
    parameters:
      temperature: 0.7
      top_p: 0.95
      max_tokens: 2048
    on_policy: false

  # Output configuration
  output:
    mode: "local_file"
    local_path: "./output/zai_result.jsonl"
    format: "jsonl"

  metadata:
    experiment_name: "zai_glm_distillation"
    model: "glm-4.6"
    description: "Distillation using Z.AI GLM models"
